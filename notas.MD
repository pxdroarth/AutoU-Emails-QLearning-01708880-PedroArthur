## introdu√ß√£o / contexto

curiosidade: fazer a m√°quina aprender com o erro

antes: classificador fixo, aprendia uma vez s√≥

agora: sistema vivo, aprende com feedback

refor√ßo = tentativa, erro e recompensa

## conceito central

aprende por tentativa e erro

recompensa: +1 acerto / -1 erro

cada e-mail √© um estado

a√ß√µes: produtivo / improdutivo

n√£o recebe resposta pronta

descobre sozinho o que d√° mais resultado

o Q-Learning treina em epis√≥dios
 cada epis√≥dio √© uma varredura completa no conjunto de exemplos (ou uma sequ√™ncia de intera√ß√µes)

dentro de cada epis√≥dio o agente:

l√™ um exemplo (e-mail)

decide uma a√ß√£o (produtivo/improdutivo)

recebe uma recompensa:

+1 se acertou

‚àí1 se errou

o modelo aprende de duas formas

feedback humano direto e base de dados acumulada

üß† feedback humano

cada clique em certo ou errado √© uma li√ß√£o nova

feedback vai pro csv_foruseres

entra no pr√≥ximo treino automaticamente

agente ajusta a Q-Table com base na resposta do usu√°rio

aprendizado vivo em tempo real

refor√ßo imediato curto prazo

quanto mais intera√ß√£o mais esperto ele fica

## base de dados

o train.csv √© o conhecimento consolidado

garante que o agente n√£o esquece o que j√° aprendeu

toda rodada de treino revisita a base completa

refor√ßo acumulado longo prazo

mant√©m equil√≠brio entre aprender e relembrar

combina√ß√£o de experi√™ncia antiga com novas corre√ß√µes

## aprendizado h√≠brido

feedback ensina o instante

base consolida o aprendizado

juntos o modelo evolui de forma cont√≠nua

o sistema aprende com o uso e se fortalece com os dados

um ciclo constante entre a√ß√£o erro corre√ß√£o e mem√≥ria

## transforma√ß√£o do modelo

come√ßou com regress√£o log√≠stica (supervisionado)

desafio: sair do aprendizado fixo

ideia: sem professor, aprende com intera√ß√£o

entrou o Q-Learning

Q-Table salva no SQLite

## Diferen√ßa entre SQLite e CSV no projeto AutoU

CSV = base de dados brutos (exemplos de treino, feedback, r√≥tulos).
√â onde ficam os textos e as classifica√ß√µes que o agente usa como experi√™ncia de aprendizado.

SQLite = base de conhecimento aprendido (a Q-Table).
√â onde o modelo grava os resultados do aprendizado ‚Äî os valores Q(s,a) que representam o quanto vale cada decis√£o em cada estado. ou seja guarda a exploracao do qlearning oq ele aprendeu.

## implementa√ß√£o t√©cnica

guarda Q(s,a) = valor da a√ß√£o por estado

atualiza com f√≥rmula do Q-Learning

estado = tamanho + keywords

pol√≠tica Œµ-greedy: explora ou aproveita

epsilon controla explora√ß√£o

epsilon decai com o tempo

## dificuldades

Q n√£o atualizava, sistema parecia parado

solu√ß√£o: atualizar e persistir no banco

integrar feedback humano em tempo real

salvar corre√ß√µes no csv_foruseres

feedback entra no pr√≥ximo treino

## frontend ‚Üî backend

frontend simples (HTML + JS)

usu√°rio marca ‚Äúcerto‚Äù ou ‚Äúerrado‚Äù

feedback vai pra API

modelo aprende com o uso real

agente vai ficando mais humano

## m√©tricas e visualiza√ß√£o

gera accuracy, precision, recall, f1

gr√°ficos de recompensa e epsilon

epsilon ‚Üì = menos explora√ß√£o, mais confian√ßa

quero desacelerar o decay do epsilon

## recompensa por epis√≥dio  
soma das recompensas recebidas em um treino completo  
mostra se o agente est√° melhorando com o tempo  
se a curva sobe ele est√° aprendendo  
se estabiliza atingiu o √≥timo  
se cai est√° desaprendendo ou explorando demais  

## did√°tica / metodologia

constru√ß√£o por etapas

validar cada camada separada

mudar um fator e observar

aprendizado visual com gr√°ficos

ensinar aprendendo

## oq eu aprendi 

RL √© mais sobre feedback que f√≥rmula

persist√™ncia e logging = essenciais

feedback humano √© ru√≠do + ouro

ML bom continua aprendendo depois da entrega

## conclus√£o

sistema virou um aluno digital

cada corre√ß√£o = nova aula

aprende com experi√™ncia

IA √© evoluir com o erro

## frases de impacto

a recompensa √© o professor

explorar ou aproveitar? dilema da intelig√™ncia

o erro √© dado de treino

o humano ensina pelo exemplo

Q-Learning = transformar erro em conhecimento

cada clique deixa o agente mais esperto

aprendizado √© sobre feedback, n√£o s√≥ algoritmo
